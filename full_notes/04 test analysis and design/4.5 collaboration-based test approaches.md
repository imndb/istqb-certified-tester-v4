# 4.5 Collaboration-based Test Approaches

each of the **above-mentioned techniques** (see 4.2, 4.3, 4.4)
* has a particular objective with respect to *defect* detection

***collaboration-based approaches*** on top of that
* also focus on *defect* avoidance by collaboration and communication

## 4.5.1 Collaborative User Story Writing

**a user story** def.
* represents a feature that will be valuable to either a user or purchaser of a system or software
* **the 3 C's**: user stories have three critical aspects (Jeffries 2000):
  + **card**
    - the medium describing a user story
    - eg. an index card, an entry in an electronic board
  + **conversation**
    - explains how the software will be used
    - can be documented or verbal
  + **confirmation**
    - the *acceptance criteria* (see section 4.5.2)

**the most common format** for a user story:
* “As a [role], I want [goal to be accomplished], so that I can [resulting business value for the role]”,
* followed by the *acceptance criteria*

**collaborative authorship of the user story**
* can use techniques such as brainstorming & mind mapping
* the collaboration allows the team to obtain a shared vision of what should be delivered,
  + by taking into account three perspectives:
    - business,
    - development
    - testing

**good user stories should be:**
* **"INVEST"**:
  + independent,
  + negotiable,
  + valuable,
  + estimable,
  + small
  + testable
* **if a stakeholder does not know how to test** a user story,
  + this may indicate that the user story is not clear enough,
  + that it does not reflect something valuable to them,
  + that the stakeholder just needs help in testing (Wake 2003)

## 4.5.2 Acceptance Criteria

***acceptance criteria* for a user story are**
* the conditions that an implementation of the user story must meet to be accepted by stakeholders
  + from this perspective, *acceptance criteria* may be viewed as the **test conditions** that should be exercised by the tests
* usually a result of the "conversation" (see 4.5.1 above)

***acceptance criteria* are used to:**
* define the scope of the user story
* reach consensus among the stakeholders
* describe both positive and negative scenarios
* serve as a basis for the user story *acceptance testing* (see section 4.5.3)
* allow accurate planning and estimation

**two** most common **formats of writing *acceptance criteria*** for a user story:
* **scenario-oriented**
  + eg. `Given/When/Then` format used in BDD, see section 2.1.3
* **rule-oriented**
  + eg. bullet point verification list, or tabulated form of input-output mapping

most *acceptance criteria* can be **documented** in one of these two formats
* **however**, the team may use another, custom format,
  + as long as the *acceptance criteria* are well-defined and unambiguous

## 4.5.3 Acceptance Test-driven Development (ATDD)

**ATDD**:
* **a test-first approach** (see section 2.1.3)
* ***test cases* are created**
  + **prior to implementing** the user story
  + by team members **with different perspectives**
    - eg. customers, developers, and testers (Adzic 2009)
* *test cases* may be **executed manually or automated**

**step1: a specification workshop** where
* the **user story and** (if not yet defined) its ***acceptance criteria*** are
  + **analyzed**, **discussed**, and **written** by the team members
* **incompleteness, ambiguities, or *defects*** in the user story are **resolved** during this process

**step2: create the *test cases***
* can be done **by the team** as a whole **or by the tester** individually
* ***test cases* are based on the *acceptance criteria***
  + and can be seen as examples of how the software works
    - since examples and tests are the same, these terms are often used interchangeably
  + this will help the team implement the user story correctly
* during the *test design* the *test techniques* described in sections 4.2, 4.3 and 4.4 may be applied

**the *test design* process:**
* **typically, the first *test cases* are positive**,
  + confirming the correct behavior without exceptions or error conditions,
  + and comprising the sequence of activities executed if everything goes as expected
* after the positive *test cases* are done, the **team should perform negative testing**
* **finally**, the team should cover **non-functional quality characteristics** as well
  + eg. performance efficiency, usability

***test cases* should be** expressed in a way that is **understandable for the stakeholders**
* typically, *test cases* **contain sentences in natural language** involving
  + the necessary **preconditions** (if any),
  + the **inputs**,
  + the **postconditions**

**the scope of *test cases***
* they **must cover all the characteristics of the user story**
* they **should not go beyond the story**
* **however, the *acceptance criteria* may detail some** of the issues described in the user story
* in addition, **no two *test cases* should describe the same characteristics** of the user story

when captured **in a format supported by a test automation framework**,
* the developers can automate the *test cases*
  + by writing the supporting code as they implement the feature described by a user story
* the acceptance tests then become executable requirements
